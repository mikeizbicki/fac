#!/usr/bin/env python3
'''
`fac` is a build system for LLM-based agentic projects.
The Latin verb `facio` means to do/make, and fac is the imperative form.
'''

# setup logging
import logging
handler = 'standard'
#handler = 'rich'

if handler == 'rich':
    from rich.logging import RichHandler
    handler = RichHandler(
        show_time=False,
        show_path=False,
        keywords=[]
    )
    handler.setFormatter(logging.Formatter(
        '%(asctime)s %(message)s',
        '%Y-%m-%d %H:%M:%S'
    ))
else:
    handler = logging.StreamHandler()
    handler = logging.FileHandler('.log.exmachina')
    handler.setFormatter(logging.Formatter(
        '%(asctime)s [%(levelname)s] %(message)s',
        '%Y-%m-%d %H:%M:%S'
    ))
logger = logging.getLogger(__name__)
logger.addHandler(handler)
logger.propagate = False
logger.setLevel(logging.DEBUG)
#logger.setLevel(logging.INFO)

# imports
from collections import namedtuple, Counter
import copy
import glob
import json
import os
import pathlib
import string
import subprocess
import sys
import tempfile
import time

import clize
import groq
import openai

################################################################################
# helper functions
################################################################################

class LLM():

    providers = {
        'anthropic': {
            'base_url': 'https://api.anthropic.com/v1/',
            'apikey': 'ANTHROPIC_API_KEY'
            },
        'groq': {
            'base_url': 'https://api.groq.com/openai/v1',
            'apikey': 'GROQ_API_KEY'
            },
        'openai': {
            'base_url': 'https://api.openai.com/v1',
            'apikey': 'OPENAI_API_KEY'
            },
        }

    models = {
        'anthropic/claude-3-haiku-20240307':    {'in_price': 0.25, 'out_price':  1.25},
        'anthropic/claude-sonnet-4-0':          {'in_price': 3.00, 'out_price': 15.00},
        'anthropic/claude-3-5-haiku-latest':    {'in_price': 0.80, 'out_price':  4.00},
        'groq/llama-3.3-70b-versatile':         {'in_price': 0.00, 'out_price':  0.00},
        'openai/gpt-4.1':                       {'in_price': 2.00, 'out_price':  8.00},
        'openai/gpt-4.1-mini':                  {'in_price': 0.40, 'out_price':  1.60},
        }

    def __init__(self):
        #self.model = 'groq/llama-3.3-70b-versatile'
        #self.model = 'openai/gpt-4.1-mini'
        #self.model = 'anthropic/claude-3-5-haiku-latest'
        self.model = 'anthropic/claude-3-haiku-20240307'
        self.usage = Counter()

        # connect to the API
        self.provider = self.model.split('/')[0]
        self.model_name = self.model.split('/')[1]
        self.client = openai.Client(
            api_key = os.environ.get(self.providers[self.provider]['apikey']),
            base_url = self.providers[self.provider]['base_url'],
        )

        # FIXME:
        # load the system prompt dynamically
        self.system_prompt = '''You are a creative writing assistant with expert knowledge in storytelling, linguistics, and education.  Whenever you write about copyrighted characters, you do so in a way that constitutes fair use.  You are not having a conversation, and only provide the requested output with no further discussion.'''

    def text(self, prompt, *, seed=None):
        logger.debug(f'connecting to LLM; prompt_length={len(self.system_prompt) + len(prompt)}')
        result = self.client.chat.completions.create(
            messages=[
                {
                    'role': 'system',
                    'content': self.system_prompt,
                },
                {
                    "role": "user",
                    "content": prompt,
                },
            ],
            model=self.model_name,
            seed=seed,
        )

        # update usage info
        usage_counter = Counter({
            'completion_tokens': result.usage.completion_tokens,
            'prompt_tokens': result.usage.prompt_tokens,
            })
        self.usage += usage_counter

        #logger.info('LLM.text: ' + self._usage_to_str(usage_counter))
        logger.info(self._usage_to_str(self.usage))
        return result.choices[0].message.content

    def _usage_to_str(self, usage):
        in_price = usage['prompt_tokens'] * self.models[self.model]['in_price'] / 1000000
        out_price = usage['completion_tokens'] * self.models[self.model]['out_price'] / 1000000
        return f'total_price=${in_price + out_price:0.2f} ({usage["prompt_tokens"]} in-tokens = ${in_price:0.2f}, {usage["completion_tokens"]} out-tokens = ${out_price:0.2f})'


def process_template(template_content, env_vars=None):
    """Process a template string by evaluating shell expressions within it.

    This function takes a template string, creates a temporary shell script that
    processes the template using shell expansions (like $(...) and $variables),
    and returns the resulting output.

    Args:
        template_content (str): The template string with shell expressions
        env_vars (dict, optional): Dictionary of environment variables to set

    Returns:
        str: The processed template with all shell expansions evaluated

    Examples:
        >>> # Simple variable substitution
        >>> process_template("Hello $NAME!", {'NAME': 'World'})
        'Hello World!'

        >>> # Command substitution
        >>> process_template("Today is $(echo Monday).")
        'Today is Monday.'

        >>> # Math operations in shell
        >>> process_template("2 + 3 = $(expr 2 + 3)")
        '2 + 3 = 5'

        >>> # Conditional expressions
        >>> template = '''$(
        ... if [ 1 -eq 1 ]; then
        ...   echo "True"
        ... else
        ...   echo "False"
        ... fi
        ... )'''
        >>> process_template(template)
        'True'

        >>> # Error in shell code: unmatched paren
        >>> process_template("2 + 3 = $(expr 2 + 3")  # doctest: +IGNORE_EXCEPTION_DETAIL
        Traceback (most recent call last):
        ...
        TemplateProcessingError: ...

        >>> # Error in shell code: using a var that doesn't exist
        >>> process_template("blah blah $_UNDEFINED_VAR")  # doctest: +IGNORE_EXCEPTION_DETAIL
        Traceback (most recent call last):
        ...
        TemplateProcessingError: ...

        >>> # WARNING:
        >>> # internally, this function uses the shell's heredoc feature;
        >>> # errors from within subshells are not propagated within heredocs;
        >>> # so by default the following command would not generate an error;
        >>> # but we want it to generate an error, so we capture stderr,
        >>> # and throw an error whenever stderr is non-empty;
        >>> # this gets the correct behavior for the following command
        >>> process_template("blah blah $(echo $_UNDEFINED_VAR)")  # doctest: +IGNORE_EXCEPTION_DETAIL
        Traceback (most recent call last):
        ...
        TemplateProcessingError: blah
        >>> # the downside of this approach is that
        >>> # non-erroring commands that write to stderr will generate template errors
        >>> process_template("blah blah $(echo blah >&2)")  # doctest: +IGNORE_EXCEPTION_DETAIL
        Traceback (most recent call last):
        ...
        TemplateProcessingError: blah
    """
    if env_vars is None:
        env_vars = {}

    # Create a temporary shell script
    fd, script_path = tempfile.mkstemp(suffix='.sh')
    try:
        # Close the file descriptor returned by mkstemp
        os.close(fd)

        # Write to the file using a regular file handle
        with open(script_path, 'w') as script:
            # Write a shell script that will output the processed template
            script.write('#!/bin/bash\n')
            script.write('set -e\n')  # Exit immediately if a command exits with non-zero status
            script.write('set -u\n')  # Treat unset variables as an error

            # Use cat with a heredoc to process the template
            script.write('cat << __EOF_DELIMITER_END\n')
            script.write(template_content)
            script.write('\n__EOF_DELIMITER_END\n')

        # Make the script executable
        os.chmod(script_path, 0o755)

        # Execute the script and capture output
        result = subprocess.run([script_path], capture_output=True, text=True, env={**os.environ, **env_vars})
        if result.returncode != 0 or len(result.stderr.strip()) > 0:
            raise TemplateProcessingError(result.returncode, result.stdout, result.stderr)
        return result.stdout.strip()

    finally:
        # Ensure the temporary file is removed
        if os.path.exists(script_path):
            os.unlink(script_path)


class TemplateProcessingError(Exception):
    """Exception raised when template processing fails."""

    def __init__(self, returncode, stdout, stderr):
        self.returncode = returncode
        self.stdout = stdout
        self.stderr = stderr
        super().__init__(stderr)


class CommandExecutionError(Exception):
    def __init__(self, result):
        self.result = result
        super().__init__(result.stderr)


class VariableEvaluationError(Exception):
    def __init__(self, var, expr, context, result):
        errorstrs = [
            f'error evaluating {var}=$({expr})',
            f'context={context}',
            f"result.returncode={result.returncode}",
            f"result.stdout={result.stdout}",
            f"result.stderr={result.stderr}",
            ]
        super().__init__('\n'.join(errorstrs))


class EmptyVariableError(Exception):
    def __init__(self, var, expr):
        errorstrs = [
            f'{var}=$({expr})',
            ]
        super().__init__('\n'.join(errorstrs))


def expand_path(path, env_vars=None):
    """
    Expand environment variables and wildcards in a path.

    Args:
        path (str): The path with potential environment variables and wildcards
        env_vars (dict, optional): Dictionary of environment variables to use

    Returns:
        list: List of expanded paths

    The following example creates a tempdir and places two files inside of it.
    Then the `expand_path` function is used to list those files.
    The output is wrapped in `len` because the output paths are non-deterministic.

    >>> import tempfile
    >>> with tempfile.TemporaryDirectory() as tmpdir:
    ...     test_env = {'PY_TEST_VAR': tmpdir}
    ...     open(os.path.join(tmpdir, 'test1.txt'), 'w').close()
    ...     open(os.path.join(tmpdir, 'test2.txt'), 'w').close()
    ...     len(expand_path('$PY_TEST_VAR/*.txt', test_env))
    2

    If the input string uses an environment variable that is undefined,
    then a `TemplateProcessingError` will be raised.

    >>> with tempfile.TemporaryDirectory() as tmpdir:
    ...     expand_path('$PY_TEST_VAR2/*.txt', {}) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
    ...
    TemplateProcessingError: ...
    """
    # Use process_template to handle environment variable expansion
    expanded_path = process_template(path, env_vars)

    # Handle wildcards with glob
    paths = glob.glob(expanded_path)
    paths = [str(pathlib.Path(path).resolve()) for path in paths]
    paths = [os.path.relpath(path) for path in paths]
    return paths


def match_pattern(patterns, input_string):
    """
    Match an input string against a list of patterns and extract variables.

    Args:
        patterns: List of pattern strings with variables like "$SERIES/$STORY/outline.json"
        input_string: String to match against patterns, e.g. "a/b/outline.json"
                     If input_string contains variables like $STORY, no extraction is done for those

    Returns:
        Tuple of (matched_pattern, extracted_variables) or (None, {}) if no match

    Raises:
        TemplateProcessingError: If multiple patterns match the input string (ambiguous patterns)

    Examples:

        >>> patterns = ["$SERIES/$STORY/outline.json"]
        >>> match_pattern(patterns, "a/b/outline.json")
        ('$SERIES/$STORY/outline.json', {'SERIES': 'a', 'STORY': 'b'})

        >>> patterns = ["$SERIES/$STORY/chapter$CHAPTER/chapter.json"]
        >>> match_pattern(patterns, "mystory/adventure/chapter3/chapter.json")
        ('$SERIES/$STORY/chapter$CHAPTER/chapter.json', {'SERIES': 'mystory', 'STORY': 'adventure', 'CHAPTER': '3'})

        >>> patterns = ["$SERIES/characters/$CHARACTER/about.json"]
        >>> match_pattern(patterns, "starwars/characters/luke/about.json")
        ('$SERIES/characters/$CHARACTER/about.json', {'SERIES': 'starwars', 'CHARACTER': 'luke'})

        >>> patterns = ["$SERIES/$STORY/outline.json", "$SERIES/$STORY/locations.json"]
        >>> match_pattern(patterns, "a/b/locations.json")
        ('$SERIES/$STORY/locations.json', {'SERIES': 'a', 'STORY': 'b'})

    If `input_string` does not match any patterns,
    then we return `(None, {})`.

        >>> patterns = ["$SERIES/$STORY/outline.json"]
        >>> match_pattern(patterns, "a/b/c/outline.json")
        (None, {})

        >>> patterns = ["$SERIES/$STORY/outline.json"]
        >>> match_pattern(patterns, "a/b/summary.json")
        (None, {})

    If there are `.` references to the current directory,
    we should still match the pattern.

        >>> patterns = ['$PROJECT/outline.json', '$PROJECT/$LEVEL1/blurb.json']
        >>> match_pattern(patterns, 'test_project/outline.json')
        ('$PROJECT/outline.json', {'PROJECT': 'test_project'})

        >>> patterns = ['./$PROJECT/outline.json', '$PROJECT/$LEVEL1/blurb.json']
        >>> match_pattern(patterns, 'test_project/outline.json')
        ('$PROJECT/outline.json', {'PROJECT': 'test_project'})

        >>> patterns = ['././$PROJECT/./outline.json', '$PROJECT/$LEVEL1/blurb.json']
        >>> match_pattern(patterns, 'test_project/outline.json')
        ('$PROJECT/outline.json', {'PROJECT': 'test_project'})

        >>> patterns = ['./$PROJECT/outline.json', '$PROJECT/$LEVEL1/blurb.json']
        >>> match_pattern(patterns, './test_project/outline.json')
        ('$PROJECT/outline.json', {'PROJECT': 'test_project'})

        >>> patterns = ['./$PROJECT/./outline.json', '$PROJECT/$LEVEL1/blurb.json']
        >>> match_pattern(patterns, './test_project/outline.json')
        ('$PROJECT/outline.json', {'PROJECT': 'test_project'})

        >>> patterns = ['$PROJECT/./outline.json', '$PROJECT/$LEVEL1/blurb.json']
        >>> match_pattern(patterns, './test_project/outline.json')
        ('$PROJECT/outline.json', {'PROJECT': 'test_project'})

    If there are multiple patterns that could match,
    then the choice of pattern is ambiguous.
    Raise a ValueError.
    This likely indicates a problem with the structure of the dependencies in the config.

        >>> patterns = ["$A/$B/$C/file.json", "$X/something/$Y/file.json"]
        >>> match_pattern(patterns, "first/something/second/file.json")
        Traceback (most recent call last):
            ...
        ValueError: Ambiguous pattern match for 'first/something/second/file.json'

    If we pass a variable in the `input_string`,
    we should not match that variable to one of the patterns in the returned variable list.

        >>> patterns = ["$SERIES/$STORY/outline.json"]
        >>> match_pattern(patterns, "a/$STORY/outline.json")
        ('$SERIES/$STORY/outline.json', {'SERIES': 'a'})

        >>> patterns = ["$SERIES/$STORY/chapter$CHAPTER/chapter.json"]
        >>> match_pattern(patterns, "a/b/chapter$CHAPTER/chapter.json")
        ('$SERIES/$STORY/chapter$CHAPTER/chapter.json', {'SERIES': 'a', 'STORY': 'b'})

        >>> patterns = ["$SERIES/$STORY/chapter$CHAPTER/chapter.json"]
        >>> match_pattern(patterns, "$SERIES/$STORY/chapter$CHAPTER/chapter.json")
        ('$SERIES/$STORY/chapter$CHAPTER/chapter.json', {})

        >>> patterns = ["$SERIES/$STORY/chapter$CHAPTER/chapter.json"]
        >>> match_pattern(patterns, "$SERIES/b/chapter$CHAPTER/chapter.json")
        ('$SERIES/$STORY/chapter$CHAPTER/chapter.json', {'STORY': 'b'})
    """
    import re
    import os

    # Normalize input string by removing './' references
    norm_input = re.sub(r'(\.\/)+', '', input_string)

    # Create a mapping of normalized patterns to original patterns
    norm_to_orig = {}
    normalized_patterns = []

    for pattern in patterns:
        # Normalize pattern by removing './' references
        norm_pattern = re.sub(r'(\.\/)+', '', pattern)
        normalized_patterns.append(norm_pattern)
        norm_to_orig[norm_pattern] = pattern

    matched_patterns = []
    matched_vars = []

    input_segments = norm_input.split('/')

    for norm_pattern in normalized_patterns:
        pattern_segments = norm_pattern.split('/')

        # Skip patterns with different number of segments
        if len(pattern_segments) != len(input_segments):
            continue

        variables = {}
        is_match = True

        for i, (p_seg, i_seg) in enumerate(zip(pattern_segments, input_segments)):
            # Check if pattern segment contains variables
            if '$' in p_seg:
                # Convert pattern segment to regex
                regex = '^'
                pos = 0
                var_names = []

                while pos < len(p_seg):
                    if p_seg[pos] == '$':
                        # Found start of a variable
                        var_start = pos + 1
                        var_end = var_start
                        while var_end < len(p_seg) and (p_seg[var_end].isalnum() or p_seg[var_end] == '_'):
                            var_end += 1

                        var_name = p_seg[var_start:var_end]
                        var_placeholder = f"${var_name}"

                        # Check if this variable appears in input segment
                        if var_placeholder in i_seg:
                            # Match literally
                            regex += re.escape(var_placeholder)
                        else:
                            # Capture the variable value
                            var_names.append(var_name)
                            regex += '(.*?)'

                        pos = var_end
                    else:
                        # Add regular character to regex
                        if p_seg[pos] in '.^$*+?{}[]\\|()':
                            regex += '\\'
                        regex += p_seg[pos]
                        pos += 1

                regex += '$'

                # Apply regex to input segment
                match = re.match(regex, i_seg)

                if not match:
                    is_match = False
                    break

                # Extract captured variables
                for j, var_name in enumerate(var_names):
                    variables[var_name] = match.group(j+1)

            elif p_seg != i_seg:
                # Literal segments must match exactly
                is_match = False
                break

        if is_match:
            matched_patterns.append(norm_pattern)
            matched_vars.append(variables)

    if len(matched_patterns) > 1:
        raise ValueError(f"Ambiguous pattern match for '{input_string}'")

    if matched_patterns:
        # For tests, we should return the clean version of the pattern
        return (matched_patterns[0], matched_vars[0])
    else:
        return (None, {})


################################################################################
# main function
################################################################################

class BuildSystem:

    def __init__(
            self,
            project_dir,
            pattern_to_build='/dev/stdout',
            *,
            prompt_dir='prompts',
            config_file='fac.yaml',
            overwrite=False,
            print_prompt=False,
            print_contexts=False,
            simple_logger=False,
            quiet=False,
            collapse_display=True,
            ):

        # update attributes;
        # these are all the command line args from clize
        self.pattern_to_build = pattern_to_build
        self.prompt_dir = prompt_dir
        self.overwrite = overwrite
        self.print_prompt = print_prompt
        self.print_contexts = print_contexts
        self.collapse_display = collapse_display

        # all paths will be relative to project_dir
        os.chdir(project_dir)

        # load config file
        import yaml
        config_path = config_file
        with open(config_path) as fin:
            self.full_config = yaml.safe_load(fin)

        self.llm = LLM()

        # setup logging
        from rich.tree import Tree
        from rich.console import Console
        from rich.live import Live
        from rich.align import Align
        from rich.traceback import install
        import traceback
        #install()
        self.logtree = Tree(f'build_pattern("{pattern_to_build}")')
        self.live = None
        if not quiet:
            if simple_logger:
                handler = logging.StreamHandler()
                handler.setFormatter(logging.Formatter(
                    '%(asctime)s [%(levelname)s] %(message)s',
                    '%Y-%m-%d %H:%M:%S'
                ))
                logger = logging.getLogger(__name__)
                logger.addHandler(handler)
            else:
                self.live = Live(
                        self.logtree,
                        vertical_overflow='visible',
                        refresh_per_second=4,
                        )
                self.live.start()

        try:
            if pattern_to_build:
                self.build_pattern(pattern_to_build, {}, self.logtree)
            self.live and self.live.stop()
        except Exception as e:
            self.live and self.live.stop()
            raise e


    def build_pattern(self, pattern_to_build, input_env, subtree):
        logger.debug(f'pattern_to_build="{pattern_to_build}"')

        # load pattern config
        config_patterns = self.full_config.keys()
        transformed_pattern, pattern_env = match_pattern(config_patterns, pattern_to_build)
        subtree.add(f'input_env={input_env}; pattern_env={pattern_env}')
        logger.debug(f"transformed_pattern={transformed_pattern}; pattern_env={pattern_env}")
        assert transformed_pattern, f'transformed_pattern={transformed_pattern}, pattern_to_build={pattern_to_build}, input_env={input_env}, config_patterns={config_patterns}'
        assert transformed_pattern in self.full_config
        config = self.full_config[transformed_pattern]

        # compute the variables and resolve dependencies
        #deptree = subtree.add(f'resolving dependencies and variables')
        deptree = subtree
        BuildContext = namedtuple('Context', [
            'variables',
            'include_paths',
            'unresolved_dependencies'
            ])
        contexts = [BuildContext(
            {**input_env, **pattern_env},
            [],
            config['dependencies'].split(),
            )]
        
        DUMMY_VAR = 'DUMMY_VAR_CREATED_TO_FORCE_LOOP_TO_RUN_ONCE'
        for var, expr in config.get('variables', {DUMMY_VAR: 'DUMMY_VAL'}).items():
            expr = expr.strip()

            # compute the variables
            logger.debug(f'computing {var}=$({expr})')
            contexts0 = contexts
            contexts = []
            for context in contexts0:

                # compute the dependencies
                logger.debug(f'computing dependencies for "{pattern_to_build}"')
                include_paths1 = []
                unresolved_dependencies1 = []
                for dep_pattern in context.unresolved_dependencies:
                    subdeptree = deptree.add(f'resolving dependency: "{dep_pattern}"')
                    try:
                        dep_paths = expand_path(dep_pattern, context.variables)
                        if not dep_paths:
                            logger.debug(f'no paths found for dep_pattern={dep_pattern}, building')
                            self.build_pattern(dep_pattern, context.variables, subdeptree)
                        else:
                            logger.debug(f'matched: dep_pattern="{dep_pattern}"')
                        include_paths1.extend(dep_paths)

                    # if expand_path failed, there is an unresolved dependency in the path;
                    # so we must keep it around and try to resolve it again after computing more variables
                    except TemplateProcessingError:
                        if dep_pattern in self.full_config:
                            self.build_pattern(dep_pattern, context.variables, subdeptree)
                        else:
                            unresolved_dependencies1.append(dep_pattern)
                    # update the tree to indicate success
                    #print(f"subdeptree={subdeptree}")
                    if self.live and self.collapse_display:
                        deptree.children.remove(subdeptree)
                        deptree.add(f'resolved dependency {dep_pattern}')

                logger.debug(f"include_paths1={include_paths1}")
                logger.debug(f"unresolved_dependencies1={unresolved_dependencies1}")

                # do not evaluate the variable if it is DUMMY_VAR,
                # since it was created only to force the unresolved_dependencies to run once
                if var == DUMMY_VAR:
                    vals = ''

                # the var was specified in the environment,
                # so we do not execute the expr
                elif var in context.variables:
                    vals = context.variables[var]

                # run the expr in the bash shell
                else:
                    full_command = "set -eu; " + expr
                    result = subprocess.run(
                        full_command,
                        shell=True,
                        capture_output=True,
                        text=True,
                        executable="/bin/bash",
                        env=context.variables,
                        )
                    if result.returncode != 0 or result.stdout.strip() == '':
                        raise VariableEvaluationError(var, expr, context, result)
                    vals = result.stdout

                    if vals.strip() == '':
                        raise EmptyVariableError(var, expr, result)

                # lists are separated by null characters;
                # for each entry in the list,
                # we will add a new context with the entry added
                vals_split = [val.strip() for val in vals.split('\0')]

                # don't add val to the contexts list when it is empty;
                # this is because when doing the split on \0,
                # we will always have the last entry be '',
                # because of the tr '\n' '\0' command
                # and all outputs ending in a '\n'
                vals_split = [val for val in vals_split if len(val) > 0]
                if len(vals_split) == 0:
                    vals_split = ['']

                for val in vals_split:

                    # if val is an integer, prepend it with zeros
                    try:
                        intval = int(val)
                        val = f'{intval:04d}'
                    except ValueError:
                        pass

                    # add the context
                    context1 = BuildContext(
                        {**context.variables, var: val},
                        context.include_paths + include_paths1,
                        unresolved_dependencies1
                        )
                    contexts.append(context1)

            if var != DUMMY_VAR:
                vals_split_disp = vals_split
                if len(vals_split) == 1:
                    vals_split_disp = vals_split[0]
                deptree.add(f'resolved variable {var}')

        # print contexts debug information
        if self.print_contexts:
            import pprint
            print('contexts=')
            pprint.pprint(contexts)
            return

        # loop over each context and run the processing code for the context
        for i, context in enumerate(contexts):

            # after generating dependencies
            path_to_generate = process_template(pattern_to_build, context.variables)
            infostr = f'building {i+1}/{len(contexts)} "{path_to_generate}"'
            buildtree = subtree.add(infostr)
            logger.info(infostr)
            logger.debug(f'context={context}')

            # create output directory if needed
            logger.debug(f"path_to_generate={path_to_generate}")
            dirname = os.path.dirname(path_to_generate)
            if len(dirname) > 0:
                os.makedirs(dirname, exist_ok=True)

            # build with a custom command
            if config.get('cmd'):
                buildtree.add(config['cmd'])

                result = subprocess.run(
                    config['cmd'],
                    shell=True,
                    capture_output=True,
                    text=True,
                    executable="/bin/bash",
                    env=context.variables,
                    )
                print(result.stdout)
                if result.returncode != 0:
                    raise CommandExecutionError(result)

            # build the target with the LLM
            else:
                buildtree.add('llm.text()')
                self.context_to_file(path_to_generate, config, context)

            # validate file
            self.validate_file(path_to_generate)

            # update tree
            if self.collapse_display and self.live:
                buildtree.children = []

        if self.collapse_display and self.live and subtree is not self.logtree:
            subtree.children = []

    def validate_file(self, path, fix_json=True):
        # ensure the input path exists
        if not os.path.exists(path):
            raise RuntimeError(f'path="{path}" does not exist')
        
        # ensure the file is non-empty
        if not path.startswith('/dev/') and os.path.getsize(path) == 0:
            raise RuntimeError(f'os.path.getsize("{path}")=0')

        # if the file is JSON, ensure that it is valid JSON
        _, extension = os.path.splitext(path)
        if extension == '.json':
            with open(path) as fin:
                text = fin.read()
            try:
                json.loads(text)
            except json.JSONDecodeError as e:
                if fix_json:
                    print('fixing JSONDecodeError')
                    import json_repair
                    with open(path, 'wt') as fout:
                        obj = json_repair.loads(text, skip_json_loads=True)
                        json.dump(obj, fout)

    def context_to_file(self, path_to_generate, config, context):

        # compute files_prompt
        files_prompt = '<documents>\n'
        for path in context.include_paths:
            # FIXME:
            # when piping into stdin, open('/dev/stdin') fails because the open function does not work on pipe "files";
            # this is a hackish way to detect that we're piping into stdin,
            # and then changing path to a value that is compatible with open;
            # in theory, weirdly named files could break this hack
            if 'pipe:[' in path: 
                path = 0
            with open(path) as fin:
                files_prompt += f'''<document path="{path}">\n{fin.read().strip()}\n</document>\n'''
        files_prompt += '</documents>'

        # compute the instructions prompt
        filename = os.path.basename(path_to_generate)
        _, extension = os.path.splitext(filename)

        if 'prompt' in config:
            prompt_cmd = config['prompt']
        else:
            if 'prompt_file' in config:
                prompt_path = config['prompt_file']
            else:
                prompt_path = os.path.join(self.prompt_dir, filename)
            with open(prompt_path) as fin:
                prompt_cmd = fin.read()
        prompt_cmd = process_template(prompt_cmd, env_vars=context.variables)
        
        if extension == '.json':
            format_cmd = '<formatting>JSON with no markdown codeblocks.</formatting>\n'
        else:
            format_cmd = ''

        # compute full prompt
        prompt = f'''<instructions>\n{prompt_cmd}\n</instructions>\n{format_cmd}{files_prompt}'''

        # stop processing if printing the prompt
        if self.print_prompt:
            print(prompt)
            return

        # write to the output file
        mode = 'wt' if self.overwrite else 'xt'
        try:
            text = self.llm.text(prompt)
            with open(path_to_generate, mode) as fout:
                fout.write(text)
        except FileExistsError:
            logger.warning(f'file "{path_to_generate}" exists; skipping')


if __name__ == '__main__':
    clize.run(BuildSystem)
